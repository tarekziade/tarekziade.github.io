<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-09-01T14:53:28+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Tarek Ziadé</title><subtitle>Notes on Building Sofware</subtitle><entry><title type="html">Building the Model Behind PDF.js Alt Text</title><link href="http://localhost:4000/2025/09/01/first-machine-learning/" rel="alternate" type="text/html" title="Building the Model Behind PDF.js Alt Text" /><published>2025-09-01T00:00:00+02:00</published><updated>2025-09-01T00:00:00+02:00</updated><id>http://localhost:4000/2025/09/01/first-machine-learning</id><content type="html" xml:base="http://localhost:4000/2025/09/01/first-machine-learning/"><![CDATA[<p>This was the very first end-to-end machine learning project with modern LLMs
in Firefox: building alt text generation our PDF.js viewer in Firefox. 
It became the foundation for many of the AI features we are now bringing into the browser.</p>

<p>In this post, I will not talk about the runtime inside Firefox, but instead
focus on the model itself. I will share the journey step by step: from
fine-tuning the model and addressing biased data, to building a validation app
and a feedback loop that grew into a full pipeline for continuous improvement.</p>

<h2 id="starting-from-an-existing-model">Starting from an Existing Model</h2>

<p>I began with a pre-trained ViT + GPT-2 model—compact enough to run locally, yet
effective for generating image descriptions. The reason I picked this model
initially was because when we first started experimenting with AI in Firefox,
transformers.js was the runtime we initialy chose. It came with an example project 
for image-to-text that used a very similar architecture, so it was a natural
starting point.</p>

<p>My first idea was to make the model even smaller so it could run more easily in
the browser. I swapped out the GPT-2 text decoder with a distilled version and
then retrained it. For training, I used popular datasets like COCO and
Flickr-30k, and at first the results looked promising. The model worked
surprisingly well given its compact size.</p>

<h2 id="wrestling-with-biased-data">Wrestling with Biased Data</h2>

<p>But soon I hit some important limitations. The human annotations in those
datasets often carried biases: gender stereotypes (“a man riding a
skateboard” even when the subject was not clearly a man), or descriptions that
leaned into cultural assumptions. In some cases, I even noticed fatphobic
language creeping into the labels. On top of that, the smaller architecture
meant the model sometimes produced clunky or imprecise captions.</p>

<h2 id="rebuilding-better-data">Rebuilding Better Data</h2>

<p>To address this, I focused on creating a cleaner dataset derived from COCO and
Flickr-30k. I wanted to preserve the diversity of images but replace the biased
human labels with synthetic, one-sentence descriptions generated by a large
language model.</p>

<p>At first, we experimented with doing this manually, but it quickly became clear
that large models, when prompted carefully, could produce captions that were
reliable, inexpensive, and incredibly fast to generate. The resulting
annotations were shorter, more neutral, and far better suited for a smaller
architecture. This new dataset helped reduce bias and improve accuracy, giving
me a strong foundation for the next rounds of fine-tuning.</p>

<h2 id="building-a-human-in-the-loop-app">Building a Human-in-the-Loop App</h2>

<p>A major milestone was building a small app for humans to validate model
output—and feed corrections back into retraining. In the app, users see the
generated alt-text on many random images, make edits, and those corrections
become new or corrected training examples. The validation workflow was simple
but transformative: it automates dataset improvement, guiding each fine-tune
round.</p>

<p>This setup might sound similar to RLHF (Reinforcement Learning with Human
Feedback), but it is not quite the same. RLHF uses human preferences to train
a reward model that guides reinforcement learning. What I built is simpler: a
human-in-the-loop supervised pipeline. People correct the outputs directly,
and those corrections go straight into the next training dataset. It is less
complex than RLHF but highly effective for improving the model in a practical
way.</p>

<h2 id="iterative-fine-tuning">Iterative Fine-Tuning</h2>

<p>Armed with better data and validation feedback, I ran multiple rounds of
fine-tuning. The improvements were obvious: the alt-text became more accurate,
more neutral, and overall more useful. Still, I had to keep iterating to deal
with class imbalance, the problem where some categories of images were
overrepresented in the dataset, leading the model to favor them more than
others. Balancing this was key to making the model more reliable across all
kinds of content.</p>

<h2 id="what-i-learned">What I Learned</h2>

<ul>
  <li><strong>Fine-tuning is just the start</strong>: the quality of training data matters more than model size.</li>
  <li><strong>Bias creeps in quickly</strong>: annotation norms reflect cultural assumptions, and that affects models.</li>
  <li><strong>Human feedback is gold</strong>: validating and correcting output closes the loop, improving the model with real-world input.</li>
  <li><strong>Building the full pipeline is powerful</strong>: from fine-tune to validation to retrain, a full cycle lets me evolve the model with each iteration.</li>
</ul>

<h2 id="where-it-lives-now">Where It Lives Now</h2>

<p>The outcome of this journey now lives in PDF.js. Firefox can generate alt-text
for PDFs locally and privately, with the model running directly on-device so
your data never leaves your machine. There is still room to grow: further
fine-tuning, tackling class imbalance more systematically, and extending the
system to multiple languages. But as a first complete end-to-end project, this
was a strong foundation and a clear proof of what is possible.</p>

<p>On a personal note, this project showed me how much of AI work is really about
iteration, patience, and learning from mistakes. It was a reminder that the real
progress comes not from one perfect model, but from building the right process
to keep improving it.</p>

<h2 id="useful-links">Useful links</h2>

<ul>
  <li><a href="https://huggingface.co/Mozilla/distilvit">Hugging Face: The trained model</a></li>
  <li><a href="https://huggingface.co/datasets/Mozilla/flickr30k-transformed-captions-gpt4o">Hugging Face: The Flickr30 dataset with cleaner alt text</a></li>
  <li><a href="https://github.com/mozilla/distilvit">GitHub: The code to train the model</a></li>
  <li><a href="https://github.com/mozilla/checkvite">GitHub: The app to validate model output</a></li>
  <li><a href="https://github.com/mozilla/pdf.js/">GitHub: The PDF.js project</a></li>
  <li><a href="https://github.com/transformers-lab/transformers.js">transformers.js</a></li>
  <li><a href="https://blog.mozilla.org/en/firefox/firefox-ai/help-us-improve-our-alt-text-generation-model">Mozilla Blog: Help us improve our alt-text generation model</a></li>
  <li><a href="https://hacks.mozilla.org/2024/05/experimenting-with-local-alt-text-generation-in-firefox-nightly">Mozilla Hacks: experimenting with Local Alt-Text Generation in Firefox Nightly</a></li>
</ul>]]></content><author><name>Tarek Ziade</name></author><category term="AI" /><category term="Machine Learning" /><category term="Firefox" /><category term="PDF.js" /><category term="Accessibility" /><category term="Local AI" /><summary type="html"><![CDATA[This was the very first end-to-end machine learning project with modern LLMs in Firefox: building alt text generation our PDF.js viewer in Firefox. It became the foundation for many of the AI features we are now bringing into the browser.]]></summary></entry><entry><title type="html">My Vision for AI and the Web</title><link href="http://localhost:4000/2025/09/01/vision-for-ai/" rel="alternate" type="text/html" title="My Vision for AI and the Web" /><published>2025-09-01T00:00:00+02:00</published><updated>2025-09-01T00:00:00+02:00</updated><id>http://localhost:4000/2025/09/01/vision-for-ai</id><content type="html" xml:base="http://localhost:4000/2025/09/01/vision-for-ai/"><![CDATA[<p>The way we use the web is shifting fast. Some people are still reluctant to
embrace AI, but the reality in 2025 is that it is here to stay. The right
response is not to reject it, but to make the best of it and shape it in ways
that serve us. Search indices are fading, and chatbots are taking their place.
Instead of browsing links, we are asking questions and getting answers directly.
That is convenient, but it also moves control away from users and into the hands
of a few AI providers.</p>

<h2 id="the-browsers-new-role">The Browser’s New Role</h2>

<p>Browsers have always been the gateway to the web, but for a long time they have
stayed neutral. Now, with AI changing how we find and use information, I believe
the browser should become more active. That is why we have started adding local
AI features in Firefox that respect privacy by design. For example, the PDF.js
alt text generator and the Smart Tab feature both run on small models that we
trained, which are downloaded to your device and executed locally. Your data
never leaves your machine. This is the kind of approach that keeps people in
control, instead of handing that control over to centralized platforms.</p>

<h2 id="the-hybrid-approach">The Hybrid Approach</h2>

<p>AI will not all run locally yet. Large models still need server-side power. But
smaller, specialized models can run on your device today, privately and
securely. That is the balance I believe in: local AI for lightweight,
privacy-first features, and server AI only when the workload is too heavy. On
top of that, privacy on the server side is improving quickly thanks to
technologies like GPU enclaves, for example Nvidia’s Confidential Compute. As
devices get stronger and these safeguards mature, more intelligence can shift
back to the user’s side with stronger guarantees at every step.</p>

<h2 id="the-future">The Future</h2>

<p>The path forward is clear to me. We need to build powerful hybrid-based
features, but always with deep care for security and privacy. That is how we
make AI in the browser truly serve the people who use it.</p>]]></content><author><name>Tarek Ziade</name></author><category term="AI" /><category term="Web" /><category term="Firefox" /><category term="Privacy" /><category term="Browsers" /><summary type="html"><![CDATA[The way we use the web is shifting fast. Some people are still reluctant to embrace AI, but the reality in 2025 is that it is here to stay. The right response is not to reject it, but to make the best of it and shape it in ways that serve us. Search indices are fading, and chatbots are taking their place. Instead of browsing links, we are asking questions and getting answers directly. That is convenient, but it also moves control away from users and into the hands of a few AI providers.]]></summary></entry></feed>