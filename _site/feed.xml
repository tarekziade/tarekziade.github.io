<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-12-05T11:47:04+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Tarek Ziadé</title><subtitle>Notes on Building Sofware</subtitle><entry><title type="html">Two Years of Building AI in Firefox</title><link href="http://localhost:4000/2025/12/05/two-years-of-ai-at-mozilla/" rel="alternate" type="text/html" title="Two Years of Building AI in Firefox" /><published>2025-12-05T00:00:00+01:00</published><updated>2025-12-05T00:00:00+01:00</updated><id>http://localhost:4000/2025/12/05/two-years-of-ai-at-mozilla</id><content type="html" xml:base="http://localhost:4000/2025/12/05/two-years-of-ai-at-mozilla/"><![CDATA[<p>When I started working on AI at Mozilla two years ago, I was a Python developer
with a background in web services and six months of machine learning experience
from working on the Nuclia DB project. I was not someone who had trained models
from scratch or built production ML infrastructure. Today, Firefox ships
multiple AI features that run entirely on-device, and I helped build the
infrastructure that makes that possible. This is a retrospective on what we
accomplished and what I learned along the way.</p>

<h2 id="building-the-foundation-the-ml-inference-runtime">Building the Foundation: The ML Inference Runtime</h2>

<p>The first major challenge was creating a runtime that could run machine learning
models directly in Firefox. We needed something that worked across platforms,
respected user privacy, and didn’t require sending data to external servers.</p>

<p>We built the Firefox ML inference engine on top of two core technologies: the
ONNX runtime for executing models, and Transformers.js to simplify the
inference work. The architecture we settled on uses a dedicated content process
for inference, keeping it isolated from the main browser process. Remote
Settings distributes both the runtime and model configurations, while IndexedDB
caches downloaded models locally.</p>

<p>One critical evolution was moving away from WebAssembly to run a pure C++ ONNX
runtime under Transformers.js. This shift gave us significantly better
performance and tighter integration with Firefox’s internals. Getting this
right required deep systems-level work, and I was fortunate to work with
fantastic engineers like Paul Adenot and Serge Guelton who brought the
expertise needed to make it happen.</p>

<p>This multi-process design was crucial. It gave us stability, security, and the
ability to update models without shipping new browser versions. We also created
our own model hub, giving us control over model distribution while still
supporting Hugging Face for developers who want broader model selection.</p>

<p>The API we exposed is deliberately simple. Developers create an engine instance
with a task name and model ID, then run inference either synchronously or with
streaming output. Behind the scenes, Firefox handles downloading models,
managing cache, and choosing the right backend.</p>

<h2 id="the-first-real-project-pdfjs-alt-text">The First Real Project: PDF.js Alt Text</h2>

<p>With the runtime in place, we needed a real feature to prove it worked. PDF.js
alt text generation became that first end-to-end project, and I have written
about it in detail before. But looking back now, it was more than just a
feature. It was the template for everything that came after.</p>

<p>We chose a Vision Transformer paired with a distilled GPT-2 decoder, compressed
to 180 million parameters and under 200MB on disk. The model runs in a couple
of seconds on a laptop, generates descriptions locally, and never sends your
PDF content anywhere. This shipped in Firefox 130, and it set the standard for
how we approach AI: small models, local execution, and privacy by default.</p>

<p>The harder work was not the model architecture. It was dealing with biased
training data and building a validation pipeline. COCO and Flickr30k datasets
carried gender stereotypes and cultural assumptions. We rebuilt the dataset
using GPT-4o annotations to generate cleaner, more neutral captions. Then we
built a human-in-the-loop validation app where users could correct outputs,
feeding those corrections back into retraining. That iterative cycle was what
made the model genuinely useful.</p>

<h2 id="smart-tab-management-and-beyond">Smart Tab Management and Beyond</h2>

<p>Once the runtime was stable and we had proven we could ship a real feature, the
next step was expanding to other use cases. Smart Tabs launched in Firefox 141,
bringing local AI to tab management.</p>

<p>The feature is simple: right-click a tab group, select “Suggest more tabs for
group,” and Firefox analyzes tab titles and descriptions to suggest similar
tabs. Users can accept or reject suggestions. The AI runs entirely on-device,
so your browsing data stays private.</p>

<p>This project showed that the infrastructure we built was flexible enough to
handle different tasks. Smart Tabs did not require a new runtime or a new model
distribution system—it reused what we already had. That reusability was proof
the architecture was working.</p>

<p>After Smart Tabs, we added many other small features following the same
pattern: laser-focused models running on-device for specific tasks. Each one
reinforced the core principle: AI should solve real problems without
compromising privacy. The infrastructure we built made it cheap to ship new
capabilities, and the local-first approach meant users stayed in control of
their data.</p>

<h2 id="ai-window-and-the-server-side-challenge">AI Window and the Server-Side Challenge</h2>

<p>The reality is that not all AI features can run locally. Small, specialized
models work well on-device, but larger language models (the kind that can handle
complex conversations and broad knowledge tasks) still need server-side compute.
That is where AI Window comes in.</p>

<p>Announced in November 2025, AI Window is an opt-in feature that brings a
conversational AI assistant directly into Firefox. Unlike our local features,
this required building infrastructure to support server-side inference while
maintaining Firefox’s commitment to user choice and control.</p>

<p>Over the past several months, I have been working on the server-side LLM
service and the overall architecture to make sure Firefox can reliably call
external services when needed. This meant designing APIs, handling failures
gracefully, managing rate limits, and ensuring the system could scale while
still respecting user preferences. The work was less about the models
themselves and more about building the bridge between Firefox and external AI
providers in a way that gives users real control.</p>

<p>This hybrid approach (local AI for privacy-sensitive tasks, server-side AI for
compute-intensive ones) is where the browser needs to go. But it raises
important questions about privacy.</p>

<h2 id="the-privacy-challenge-for-server-side-ai">The Privacy Challenge for Server-Side AI</h2>

<p>Local AI gives you perfect privacy: your data never leaves your device. But
when a model runs on a server, you are trusting someone else with your prompts,
your documents, and your questions. That trust model needs to change.</p>

<p>I am looking forward to industry standards around end-to-end encryption for
running LLM inference with full privacy guarantees. The technology already
exists. Flower.ai has built federated learning infrastructure with end-to-end
encryption that allows large models to run on remote GPUs while keeping user
data encrypted. Nvidia has Confidential Computing on H100 and Blackwell GPUs,
using hardware-based trusted execution environments to protect code and data
during inference. The performance overhead is minimal (often less than 5%) and
the privacy guarantees are real.</p>

<p>But here is the problem: none of this is part of the de facto OpenAI API
standard that most LLM services use today. If you want to call GPT-4 or Claude
or any major hosted model, there is no standardized way to do it with
end-to-end encryption or confidential compute guarantees. Your data goes to the
server in plaintext, and you have to trust the provider’s privacy policy.</p>

<p>My hope is that it will soon be possible to run inference on the cloud with
strong privacy guarantees as a standard feature, not a niche offering. The
hardware is ready. The cryptographic techniques exist. What we need now is for
the industry to adopt these capabilities as table stakes for AI services. Until
that happens, local AI remains the gold standard for privacy, and server-side
AI remains a compromise.</p>

<h2 id="what-made-this-possible">What Made This Possible</h2>

<p>Building AI features in a browser is not the same as building AI features in a
standalone app or a cloud service. The constraints are different. You have
limited resources, strict privacy requirements, and the need to work across
Windows, macOS, and Linux. Here is what made it work:</p>

<ul>
  <li>
    <p><strong>Starting small</strong>: We did not try to build everything at once. The first
runtime was minimal. The first model was simple. We added complexity only
when we needed it.</p>
  </li>
  <li>
    <p><strong>Privacy as a requirement, not a feature</strong>: Every decision started with “can
this run locally?” If the answer was no, we either changed the approach or
did not build it.</p>
  </li>
  <li>
    <p><strong>Reusable infrastructure</strong>: We built the runtime once and used it for
multiple features. That meant each new AI capability got cheaper to ship.</p>
  </li>
  <li>
    <p><strong>Learning from real users</strong>: The validation app for PDF.js alt text was not
just about improving the model—it was about understanding what real people
needed. User feedback drove every iteration.</p>
  </li>
</ul>

<h2 id="what-i-learned">What I Learned</h2>

<p>Two years ago, I did not know how to train a model or what ONNX was. Now I have
shipped multiple AI features in production. Here is what stuck with me:</p>

<ul>
  <li>
    <p><strong>You do not need a PhD</strong>: Machine learning has a reputation for being
inaccessible, but the tools have gotten good enough that you can learn by
doing. I started with a pre-trained model, fine-tuned it, and kept iterating.
Most of the work was engineering, not research.</p>
  </li>
  <li>
    <p><strong>Data quality beats model size</strong>: We spent more time cleaning datasets and
handling bias than we did optimizing model architecture. A smaller model
trained on better data outperformed a larger model trained on messy data.</p>
  </li>
  <li>
    <p><strong>Privacy is possible</strong>: The narrative around AI assumes everything needs to
run in the cloud. It does not. Local models work. They are fast enough, small
enough, and private by default.</p>
  </li>
  <li>
    <p><strong>Building the process matters more than building the model</strong>: The validation
pipeline, the retraining loop, the distribution system. That infrastructure
was more important than any single model.</p>
  </li>
</ul>

<h2 id="what-is-next">What is Next</h2>

<p>This work is not finished. We plan to iterate on PDF.js alt text, expand
Smart Tabs, and bring AI Window to users who want conversational AI in their
browser. WebNN is coming, and that will give us even better performance for
local models. The Firefox ML runtime is still experimental, but it is stable
enough that other teams are starting to build on it.</p>

<p>The bigger challenge is pushing the industry toward privacy-preserving
server-side AI. Confidential compute and end-to-end encryption for LLM
inference should not be experimental features. They should be the default. I
hope to see more providers adopt these technologies and for standards bodies to
make privacy guarantees a core part of the AI API specifications.</p>

<p>On a personal level, these two years showed me that AI in the browser is not
just possible—it is the right way to do it. Local models give users control.
They protect privacy. And they prove that you do not need to send your data to
a server farm to get intelligent features. But when you do need server-side
compute, it should come with strong privacy guarantees, not just promises.</p>

<p>What excites me the most is running AI locally. That is where the future of
open AI lies: not just open models and open weights, but truly open AI that runs
on your device, under your control, without gatekeepers or surveillance. The
browser is the perfect platform to make that future real.</p>

<p>I am proud of what we built. More importantly, I am excited about what comes next.</p>

<h2 id="useful-links">Useful links</h2>

<h3 id="firefox-features">Firefox Features</h3>
<ul>
  <li><a href="https://firefox-source-docs.mozilla.org/toolkit/components/ml/index.html">Firefox ML Documentation</a></li>
  <li><a href="https://blog.mozilla.org/en/firefox/ai-window/">Mozilla Blog: AI Window</a></li>
  <li><a href="https://blog.mozilla.org/en/firefox/firefox-ai/help-us-improve-our-alt-text-generation-model">Mozilla Blog: Help us improve our alt-text generation model</a></li>
  <li><a href="http://hacks.mozilla.org/2024/05/experimenting-with-local-alt-text-generation-in-firefox-nightly/">Mozilla Hacks: Experimenting with Local Alt-Text Generation</a></li>
  <li><a href="https://blog.mozilla.org/en/mozilla/heres-what-were-working-on-in-firefox/">Mozilla Blog: Here’s what we’re working on in Firefox</a></li>
  <li><a href="https://dig.watch/updates/smart-tab-management-comes-to-firefox-with-local-ai">Smart Tab Management in Firefox</a></li>
</ul>

<h3 id="privacy-preserving-ai">Privacy-Preserving AI</h3>
<ul>
  <li><a href="https://flower.ai/">Flower.ai: Federated AI Framework</a></li>
  <li><a href="https://flower.ai/intelligence/">Flower Intelligence: End-to-end encryption for AI</a></li>
  <li><a href="https://www.nvidia.com/en-us/data-center/solutions/confidential-computing/">NVIDIA Confidential Computing</a></li>
  <li><a href="https://developer.nvidia.com/blog/confidential-computing-on-h100-gpus-for-secure-and-trustworthy-ai/">NVIDIA H100 Confidential Computing for AI</a> (Performance benchmarks showing &lt;5% overhead)</li>
  <li><a href="https://arxiv.org/html/2409.03992v1">ArXiv: Confidential Computing on H100 GPU Performance Study</a></li>
</ul>]]></content><author><name>Tarek Ziade</name></author><category term="AI" /><category term="Firefox" /><category term="Machine Learning" /><category term="Privacy" /><summary type="html"><![CDATA[When I started working on AI at Mozilla two years ago, I was a Python developer with a background in web services and six months of machine learning experience from working on the Nuclia DB project. I was not someone who had trained models from scratch or built production ML infrastructure. Today, Firefox ships multiple AI features that run entirely on-device, and I helped build the infrastructure that makes that possible. This is a retrospective on what we accomplished and what I learned along the way.]]></summary></entry><entry><title type="html">WebNN is the future of browsers AI</title><link href="http://localhost:4000/2025/11/21/why-webnn-is-the-future-of-ai-in-browsers/" rel="alternate" type="text/html" title="WebNN is the future of browsers AI" /><published>2025-11-21T00:00:00+01:00</published><updated>2025-11-21T00:00:00+01:00</updated><id>http://localhost:4000/2025/11/21/why-webnn-is-the-future-of-ai-in-browsers</id><content type="html" xml:base="http://localhost:4000/2025/11/21/why-webnn-is-the-future-of-ai-in-browsers/"><![CDATA[<p>For years, running machine learning in the browser meant juggling GPU support,
WASM fallbacks, and flags. WebNN changes that by giving the web a standard
inference API between JavaScript and hardware. It is the missing piece that
turns the browser into a first-class AI client runtime.</p>

<p>Running AI locally is the long game. A decade from now laptops and phones will
run much larger models natively, and the best experiences won’t require sending
your data off to a cloud service. WebNN is how the web gets there.</p>

<h2 id="what-webnn-really-is">What WebNN really is</h2>

<p>WebNN is a W3C draft specification that exposes a graph-based neural network
API to the web platform. Instead of binding directly to CUDA or Metal, browsers
map WebNN calls to whatever native acceleration they have: DirectML on Windows,
Core ML on macOS and iOS, NNAPI on Android, or a CPU path via TFLite/XNNPACK.
When a CPU path exists, the browser can fall back there. Think of it as <code class="language-plaintext highlighter-rouge">canvas</code>
for neural networks: you provide the graph, the browser picks the fastest safe
backend.</p>

<ul>
  <li>Spec: <a href="https://www.w3.org/TR/webnn/">https://www.w3.org/TR/webnn/</a></li>
  <li>Demos: <a href="https://webmachinelearning.github.io/webnn-samples-intro/">https://webmachinelearning.github.io/webnn-samples-intro/</a></li>
</ul>

<h3 id="webnn-as-a-graph-converter">WebNN as a graph converter</h3>

<p>WebNN is a graph builder and validator. The browser takes the graph you define
in JS, converts it into a static graph aimed at one of the underlying runtimes
in the OS (DirectML, Core ML, NNAPI, TFLite/XNNPACK, or ONNX Runtime on newer
Windows), and hands it to that native library. The heavy lifting lives there:
compilation, scheduling, and kernel selection. WebNN is the portable contract
that keeps your app code unchanged while the browser targets the best backend.</p>

<p>In Chromium, WebNN uses DirectML by default on Windows and can use the
OS-shipped ONNX Runtime backend on Windows 11 24H2+, falling back to DirectML
otherwise.</p>

<h3 id="why-not-just-use-webgpu">Why not “just use WebGPU”?</h3>

<p>Libraries like ONNX Runtime Web and TF.js already use WebGPU to get more speed,
but that means treating a graphics API as an inference runtime: writing shaders,
managing bindings, and re-implementing scheduling. WebGPU is great for explicit
control; WebNN is the spec we actually want for AI, with portable graphs,
browser-managed backend choice, and no shader boilerplate.</p>

<h2 id="why-this-matters">Why this matters</h2>

<ul>
  <li><strong>Performance without flags:</strong> WebNN can route to GPU, NPU, or CPU without
developers writing backend-specific code. That means near-native throughput
for models like Whisper Tiny or Segment Anything, but delivered via a web
page.</li>
  <li><strong>Predictable portability:</strong> The standard defines ops once; browsers own the
mapping to the best hardware path they have. Apps no longer maintain separate
WebGPU and WASM code paths.</li>
  <li><strong>Battery-aware:</strong> Because browsers control the scheduling and backend choice,
they can pick energy-efficient accelerators over brute-force GPU usage on
laptops or mobile.</li>
</ul>

<h2 id="the-current-state-and-why-it-feels-real-now">The current state (and why it feels real now)</h2>

<p>Chromium-based browsers ship WebNN behind a flag, and ONNX Runtime Web can
use the WebNN execution provider when present. According to the public
implementation status (<a href="https://webmachinelearning.github.io/webnn-status/">webmachinelearning.github.io/webnn-status</a>), the
95 ops in the spec are now covered across Core ML, Windows ML/DirectML, the
WebNN execution provider for ONNX Runtime, and TFLite/XNNPACK (LiteRT) with
only a handful still in flight. That’s enough to make real apps: speech commands,
lightweight summarization, image segmentation, and style transfer.</p>

<p>The momentum is similar to what we saw with WebGPU two years ago: early adopters
can ship progressive enhancements now, and the API will solidify while hardware
vendors line up their drivers.</p>

<p>The big shift is that WebNN moves backend selection into the browser while
keeping a high-level graph API. It is closer to Core ML or DirectML than to raw
GPU programming.</p>

<h2 id="why-i-am-bullish">Why I am bullish</h2>

<p>The web wins by being portable and low friction. AI has been the missing
capability that pushed teams toward native wrappers or cloud-heavy designs.
WebNN gives us a standard, permissionless way to run meaningful AI locally in
the browser while respecting energy and privacy constraints. It unlocks the
boring path to mass adoption: no installs, instant upgrades, and enough
abstraction that developers can stay focused on UX rather than driver matrices.</p>

<p>Now is the time to experiment, measure, and ship progressive AI features. The
future of AI in browsers looks like WebNN.</p>]]></content><author><name>Tarek Ziade</name></author><category term="AI" /><category term="WebNN" /><category term="Browsers" /><summary type="html"><![CDATA[For years, running machine learning in the browser meant juggling GPU support, WASM fallbacks, and flags. WebNN changes that by giving the web a standard inference API between JavaScript and hardware. It is the missing piece that turns the browser into a first-class AI client runtime.]]></summary></entry><entry><title type="html">The Real Danger of AI-Assisted Coding</title><link href="http://localhost:4000/2025/10/11/danger-of-ai-coding/" rel="alternate" type="text/html" title="The Real Danger of AI-Assisted Coding" /><published>2025-10-11T00:00:00+02:00</published><updated>2025-10-11T00:00:00+02:00</updated><id>http://localhost:4000/2025/10/11/danger-of-ai-coding</id><content type="html" xml:base="http://localhost:4000/2025/10/11/danger-of-ai-coding/"><![CDATA[<p><strong>TL;DR:</strong> You stop learning.</p>

<p>Lately, I’ve been conducting engineering interviews—ranging from junior to
senior levels—and I’ve noticed a worrying trend: the ability to write good code
without AI assistance has dropped dramatically in the past two years. Many
candidates mentioned that “doing all the coding” was a thing of the past, that
modern engineers were embracing what I can only describe as <em>vibe coding</em>.</p>

<p>One candidate even asked if he could use AI during the interview because that’s
how “real engineers work now.”  Fair point. In principle, I agree that we should
embrace tools that boost productivity. I use AI myself—though I limit it to
lightweight code completion in Neovim. So why not allow it in interviews?</p>

<p>The real question becomes: can a candidate demonstrate that they can <em>leverage</em>
AI effectively while still adding real value—expertise, judgment, and that human
spark that turns a functional result into an excellent one?</p>

<p>Some argue the danger lies in AI hallucinations slipping through unnoticed. And
yes, that can happen. But let’s be honest: for small, focused coding tasks, the
latest Claude and GPT models are remarkably good. I rarely see bizarre or broken
output anymore. The technology has become genuinely impressive.</p>

<p>But that’s not the <em>real</em> danger.</p>

<p>The true risk of AI-assisted coding is that it slowly numbs your ability to
<strong>think deeply</strong>, to <strong>debug</strong>, to <strong>learn from trial and error</strong>. It turns off
the part of your brain that wrestles with complexity, and that’s where real
learning happens.</p>

<p>I’ve felt this firsthand. Recently, I started practicing LeetCode problems,
especially those involving <strong>dynamic programming (DP)</strong>, a topic I’ve barely
touched in my 20 years as a professional developer. DP is notoriously difficult
to grasp unless you work in a specialized domain. It forces you to think
differently: instead of breaking problems down linearly, you must think
recursively and in overlapping subproblems. It’s a full mental rewiring.</p>

<p>And it’s <em>hard</em>. I struggled a lot, and still do. My first instinct when I got
stuck? To ask AI for hints. That’s when I realized something important: my brain
was giving up. Years of fast, intuitive coding (lately supercharged by AI) had 
rewired me to avoid slow, painful learning.</p>

<p>That realization hit hard. I had become dependent—not on AI to write code, but
on it to <em>think</em> for me whenever things got uncomfortable.</p>

<p>So now, I’m forcing myself to struggle through DP problems without help. Not
because I expect to need DP at work (it’s mostly an interview skill) but because I
want to retrain my brain to handle deep, frustrating, non-linear learning again.</p>

<p>That, in my opinion, is the real danger of AI-assisted coding:<br />
<strong>it can quietly erode your ability to think deeply and grow.</strong></p>]]></content><author><name>Tarek Ziade</name></author><category term="AI" /><category term="Machine Learning" /><summary type="html"><![CDATA[TL;DR: You stop learning.]]></summary></entry><entry><title type="html">Building the Model Behind PDF.js Alt Text</title><link href="http://localhost:4000/2025/09/01/first-machine-learning/" rel="alternate" type="text/html" title="Building the Model Behind PDF.js Alt Text" /><published>2025-09-01T00:00:00+02:00</published><updated>2025-09-01T00:00:00+02:00</updated><id>http://localhost:4000/2025/09/01/first-machine-learning</id><content type="html" xml:base="http://localhost:4000/2025/09/01/first-machine-learning/"><![CDATA[<p>This was the very first end-to-end machine learning project with modern LLMs
in Firefox: building alt text generation our PDF.js viewer in Firefox. 
It became the foundation for many of the AI features we are now bringing into the browser.</p>

<p>In this post, I will not talk about the runtime inside Firefox, but instead
focus on the model itself. I will share the journey step by step: from
fine-tuning the model and addressing biased data, to building a validation app
and a feedback loop that grew into a full pipeline for continuous improvement.</p>

<h2 id="starting-from-an-existing-model">Starting from an Existing Model</h2>

<p>I began with a pre-trained ViT + GPT-2 model—compact enough to run locally, yet
effective for generating image descriptions. The reason I picked this model
initially was because when we first started experimenting with AI in Firefox,
transformers.js was the runtime we initialy chose. It came with an example project 
for image-to-text that used a very similar architecture, so it was a natural
starting point.</p>

<p>My first idea was to make the model even smaller so it could run more easily in
the browser. I swapped out the GPT-2 text decoder with a distilled version and
then retrained it. For training, I used popular datasets like COCO and
Flickr-30k, and at first the results looked promising. The model worked
surprisingly well given its compact size.</p>

<h2 id="wrestling-with-biased-data">Wrestling with Biased Data</h2>

<p>But soon I hit some important limitations. The human annotations in those
datasets often carried biases: gender stereotypes (“a man riding a
skateboard” even when the subject was not clearly a man), or descriptions that
leaned into cultural assumptions. In some cases, I even noticed fatphobic
language creeping into the labels. On top of that, the smaller architecture
meant the model sometimes produced clunky or imprecise captions.</p>

<h2 id="rebuilding-better-data">Rebuilding Better Data</h2>

<p>To address this, I focused on creating a cleaner dataset derived from COCO and
Flickr-30k. I wanted to preserve the diversity of images but replace the biased
human labels with synthetic, one-sentence descriptions generated by a large
language model.</p>

<p>At first, we experimented with doing this manually, but it quickly became clear
that large models, when prompted carefully, could produce captions that were
reliable, inexpensive, and incredibly fast to generate. The resulting
annotations were shorter, more neutral, and far better suited for a smaller
architecture. This new dataset helped reduce bias and improve accuracy, giving
me a strong foundation for the next rounds of fine-tuning.</p>

<h2 id="building-a-human-in-the-loop-app">Building a Human-in-the-Loop App</h2>

<p>A major milestone was building a small app for humans to validate model
output—and feed corrections back into retraining. In the app, users see the
generated alt-text on many random images, make edits, and those corrections
become new or corrected training examples. The validation workflow was simple
but transformative: it automates dataset improvement, guiding each fine-tune
round.</p>

<p>This setup might sound similar to RLHF (Reinforcement Learning with Human
Feedback), but it is not quite the same. RLHF uses human preferences to train
a reward model that guides reinforcement learning. What I built is simpler: a
human-in-the-loop supervised pipeline. People correct the outputs directly,
and those corrections go straight into the next training dataset. It is less
complex than RLHF but highly effective for improving the model in a practical
way.</p>

<h2 id="iterative-fine-tuning">Iterative Fine-Tuning</h2>

<p>Armed with better data and validation feedback, I ran multiple rounds of
fine-tuning. The improvements were obvious: the alt-text became more accurate,
more neutral, and overall more useful. Still, I had to keep iterating to deal
with class imbalance, the problem where some categories of images were
overrepresented in the dataset, leading the model to favor them more than
others. Balancing this was key to making the model more reliable across all
kinds of content.</p>

<h2 id="what-i-learned">What I Learned</h2>

<ul>
  <li><strong>Fine-tuning is just the start</strong>: the quality of training data matters more than model size.</li>
  <li><strong>Bias creeps in quickly</strong>: annotation norms reflect cultural assumptions, and that affects models.</li>
  <li><strong>Human feedback is gold</strong>: validating and correcting output closes the loop, improving the model with real-world input.</li>
  <li><strong>Building the full pipeline is powerful</strong>: from fine-tune to validation to retrain, a full cycle lets me evolve the model with each iteration.</li>
</ul>

<h2 id="where-it-lives-now">Where It Lives Now</h2>

<p>The outcome of this journey now lives in PDF.js. Firefox can generate alt-text
for PDFs locally and privately, with the model running directly on-device so
your data never leaves your machine. There is still room to grow: further
fine-tuning, tackling class imbalance more systematically, and extending the
system to multiple languages. But as a first complete end-to-end project, this
was a strong foundation and a clear proof of what is possible.</p>

<p>On a personal note, this project showed me how much of AI work is really about
iteration, patience, and learning from mistakes. It was a reminder that the real
progress comes not from one perfect model, but from building the right process
to keep improving it.</p>

<h2 id="useful-links">Useful links</h2>

<ul>
  <li><a href="https://huggingface.co/Mozilla/distilvit">Hugging Face: The trained model</a></li>
  <li><a href="https://huggingface.co/datasets/Mozilla/flickr30k-transformed-captions-gpt4o">Hugging Face: The Flickr30 dataset with cleaner alt text</a></li>
  <li><a href="https://github.com/mozilla/distilvit">GitHub: The code to train the model</a></li>
  <li><a href="https://github.com/mozilla/checkvite">GitHub: The app to validate model output</a></li>
  <li><a href="https://github.com/mozilla/pdf.js/">GitHub: The PDF.js project</a></li>
  <li><a href="https://github.com/transformers-lab/transformers.js">transformers.js</a></li>
  <li><a href="https://blog.mozilla.org/en/firefox/firefox-ai/help-us-improve-our-alt-text-generation-model">Mozilla Blog: Help us improve our alt-text generation model</a></li>
  <li><a href="https://hacks.mozilla.org/2024/05/experimenting-with-local-alt-text-generation-in-firefox-nightly">Mozilla Hacks: experimenting with Local Alt-Text Generation in Firefox Nightly</a></li>
</ul>]]></content><author><name>Tarek Ziade</name></author><category term="AI" /><category term="Machine Learning" /><category term="Firefox" /><category term="PDF.js" /><category term="Accessibility" /><category term="Local AI" /><summary type="html"><![CDATA[This was the very first end-to-end machine learning project with modern LLMs in Firefox: building alt text generation our PDF.js viewer in Firefox. It became the foundation for many of the AI features we are now bringing into the browser.]]></summary></entry><entry><title type="html">My Vision for AI and the Web</title><link href="http://localhost:4000/2025/09/01/vision-for-ai/" rel="alternate" type="text/html" title="My Vision for AI and the Web" /><published>2025-09-01T00:00:00+02:00</published><updated>2025-09-01T00:00:00+02:00</updated><id>http://localhost:4000/2025/09/01/vision-for-ai</id><content type="html" xml:base="http://localhost:4000/2025/09/01/vision-for-ai/"><![CDATA[<p>The way we use the web is shifting fast. Some people are still reluctant to
embrace AI, but the reality in 2025 is that it is here to stay. The right
response is not to reject it, but to make the best of it and shape it in ways
that serve us. Search indices are fading, and chatbots are taking their place.
Instead of browsing links, we are asking questions and getting answers directly.
That is convenient, but it also moves control away from users and into the hands
of a few AI providers.</p>

<h2 id="the-browsers-new-role">The Browser’s New Role</h2>

<p>Browsers have always been the gateway to the web, but for a long time they have
stayed neutral. Now, with AI changing how we find and use information, I believe
the browser should become more active. That is why we have started adding local
AI features in Firefox that respect privacy by design. For example, the PDF.js
alt text generator and the Smart Tab feature both run on small models that we
trained, which are downloaded to your device and executed locally. Your data
never leaves your machine. This is the kind of approach that keeps people in
control, instead of handing that control over to centralized platforms.</p>

<h2 id="the-hybrid-approach">The Hybrid Approach</h2>

<p>AI will not all run locally yet. Large models still need server-side power. But
smaller, specialized models can run on your device today, privately and
securely. That is the balance I believe in: local AI for lightweight,
privacy-first features, and server AI only when the workload is too heavy. On
top of that, privacy on the server side is improving quickly thanks to
technologies like GPU enclaves, for example Nvidia’s Confidential Compute. As
devices get stronger and these safeguards mature, more intelligence can shift
back to the user’s side with stronger guarantees at every step.</p>

<h2 id="open-questions">Open Questions</h2>

<p>There is still much to figure out. One of the biggest changes is that large
language models are increasingly acting as intermediaries to the web itself.</p>

<p>Instead of users directly searching, visiting sites, and making their own
judgments, LLMs are fetching, ranking, and even rewriting information before
presenting it back.</p>

<p>That raises difficult questions:</p>

<ul>
  <li>
    <p>How do we ensure that users remain in control when the LLM is doing the search
for them?</p>
  </li>
  <li>
    <p>Can the browser provide visibility into what the model retrieved and why, so
users are not left in the dark?</p>
  </li>
  <li>
    <p>What role should open standards play to ensure transparency and user choice when
AI providers add search capabilities directly into their platforms?</p>
  </li>
</ul>

<p>These are open design questions that don’t have easy answers. But they are
central to making sure AI serves people, rather than simply reshaping the web in
ways that strip away agency.</p>

<h2 id="the-future">The Future</h2>

<p>The path forward is clear to me. We need to build powerful hybrid-based
features, but always with deep care for security and privacy. That is how we
make AI in the browser truly serve the people who use it.</p>]]></content><author><name>Tarek Ziade</name></author><category term="AI" /><category term="Web" /><category term="Firefox" /><category term="Privacy" /><category term="Browsers" /><summary type="html"><![CDATA[The way we use the web is shifting fast. Some people are still reluctant to embrace AI, but the reality in 2025 is that it is here to stay. The right response is not to reject it, but to make the best of it and shape it in ways that serve us. Search indices are fading, and chatbots are taking their place. Instead of browsing links, we are asking questions and getting answers directly. That is convenient, but it also moves control away from users and into the hands of a few AI providers.]]></summary></entry></feed>